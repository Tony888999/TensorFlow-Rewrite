# Define the HamiltonJacobiBellman class
class HamiltonJacobiBellman(FBSNN):
    def __init__(self, Xi, T, M, N, D, layers):
        super().__init__(Xi, T, M, N, D, layers)

    def phi_tf(self, t, X, Y, Z):
        return tf.reduce_sum(Z ** 2, axis=1, keepdims=True)  # M x 1

    def g_tf(self, X):
        return tf.math.log(0.5 + 0.5 * tf.reduce_sum(X ** 2, axis=1, keepdims=True))  # M x 1

    def mu_tf(self, t, X, Y, Z):
        return tf.zeros_like(X)  # M x D

    def sigma_tf(self, t, X, Y):
        sqrt_2 = tf.sqrt(2.0)
        batch_size = tf.shape(X)[0]
        D = tf.shape(X)[1]
        sigma = sqrt_2 * tf.eye(D, batch_shape=[batch_size])  # M x D x D
        return sigma

# Learning rate schedule
def learning_rate_schedule(it):
    if it < 20000:
        return 1e-3
    elif it < 50000:
        return 1e-4
    elif it < 80000:
        return 1e-5
    else:
        return 1e-6

# Define the exact solution function
def g(X):
    return np.log(0.5 + 0.5 * np.sum(X ** 2, axis=2, keepdims=True))

def u_exact(t, X):
    MC = 10 ** 5
    NC = t.shape[0]
    D = X.shape[1]

    W = np.random.normal(size=(MC, NC, D))
    T_minus_t = np.abs(T - t).reshape(1, NC, 1)
    sqrt_term = np.sqrt(2.0 * T_minus_t)
    X_expanded = X.reshape(1, NC, D)
    exponent = -g(X_expanded + sqrt_term * W)
    exp_mean = np.mean(np.exp(exponent), axis=0)
    return -np.log(exp_mean[:, 0])

# Main execution block
if __name__ == "__main__":
    M = 100  # number of trajectories (batch size)
    N = 50  # number of time snapshots
    D = 100  # number of dimensions

    layers = [D + 1] + 4 * [256] + [1]

    Xi = np.zeros([1, D], dtype=np.float32)
    T = 1.0

    # Training
    model = HamiltonJacobiBellman(Xi, T, M, N, D, layers)
    model.train(N_Iter=50, learning_rate_schedule=learning_rate_schedule)

    # Fetch test data
    t_test, W_test = model.fetch_minibatch()

    # Make predictions
    X_pred, Y_pred = model.predict(Xi, t_test, W_test)

    # Compute exact solution
    Y_test = u_exact(t_test[0, :, 0], X_pred[0, :, :])

    # Compute terminal condition
    Y_test_terminal = np.log(0.5 + 0.5 * np.sum(X_pred[:, -1, :] ** 2, axis=1, keepdims=True))

    # Plot results
    plt.figure()
    plt.plot(t_test[0, :, 0], Y_pred[0, :, 0], 'b', label='Learned $u(t,X_t)$')
    plt.plot(t_test[0, :, 0], Y_test, 'r--', label='Exact $u(t,X_t)$')
    plt.plot(t_test[0, -1, 0], Y_test_terminal[0], 'ks', label='$Y_T = u(T,X_T)$')
    plt.plot([0], Y_test[0], 'ko', label='$Y_0 = u(0,X_0)$')
    plt.xlabel('$t$')
    plt.ylabel('$Y_t = u(t,X_t)$')
    plt.title('100-dimensional Hamilton-Jacobi-Bellman')
    plt.legend()
    plt.show()

    # Compute errors
    errors = np.sqrt((Y_test - Y_pred[0, :, 0]) ** 2 / (Y_test ** 2 + 1e-10))

    # Plot errors
    plt.figure()
    plt.plot(t_test[0, :, 0], errors, 'b')
    plt.xlabel('$t$')
    plt.ylabel('relative error')
    plt.title('100-dimensional Hamilton-Jacobi-Bellman')
    plt.show()
